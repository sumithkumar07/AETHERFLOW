from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime
import uuid
import logging

from models.user import User
from models.database import get_database
from routes.auth import get_current_user
from services.ai_service import AIService
from middleware.usage_tracking import get_chat_usage_dependency, estimate_tokens, estimate_response_tokens

router = APIRouter()
ai_service = AIService()
logger = logging.getLogger(__name__)

class ChatMessage(BaseModel):
    message: str
    model: Optional[str] = "codellama:13b"
    agent: Optional[str] = "developer"
    project_id: Optional[str] = None
    conversation_id: Optional[str] = None
    context: Optional[List[Dict]] = []
    agent_prompt: Optional[str] = None
    enhanced_features: Optional[Dict] = {}

class ConversationCreate(BaseModel):
    title: str
    project_id: Optional[str] = None

@router.post("/chat")
async def chat_with_ai(
    message_data: ChatMessage,
    usage_info = Depends(get_chat_usage_dependency)
):
    """Chat with ultra-fast Groq AI agent with usage tracking - PERFORMANCE OPTIMIZED"""
    try:
        # VALIDATION: Check for empty message
        if not message_data.message or not message_data.message.strip():
            raise HTTPException(status_code=400, detail={
                "error": "Invalid request",
                "message": "Message cannot be empty or contain only whitespace",
                "field": "message"
            })
        
        # VALIDATION: Check message length limits
        if len(message_data.message) > 10000:
            raise HTTPException(status_code=400, detail={
                "error": "Message too long",
                "message": "Message cannot exceed 10,000 characters",
                "field": "message",
                "current_length": len(message_data.message),
                "max_length": 10000
            })
        
        current_user = usage_info["user"]
        user_id = usage_info["user_id"]
        track_usage = usage_info["track_usage"]
        
        logger.info(f"Processing OPTIMIZED Groq AI chat request from user {user_id}")
        
        # ðŸš€ PERFORMANCE FIX: Estimate tokens asynchronously
        input_tokens = estimate_tokens(message_data.message)
        
        # Generate unique message ID
        message_id = f"msg_{uuid.uuid4().hex[:12]}"
        
        # ðŸš€ PERFORMANCE FIX: Process with optimized AI service (parallel processing)
        ai_response = await ai_service.process_message(
            message=message_data.message,
            model=message_data.model,
            agent=message_data.agent,
            context=message_data.context[-3:] if message_data.context else [],  # OPTIMIZED: Limit context to 3 messages
            user_id=user_id,
            project_id=message_data.project_id,
            stream=False  # Keep non-streaming for consistency
        )
        
        # VALIDATION: Check AI response validity
        if not ai_response or not ai_response.get("response"):
            raise HTTPException(status_code=502, detail={
                "error": "AI service error",
                "message": "AI service returned empty or invalid response",
                "suggestion": "Please try again or contact support if the issue persists"
            })
        
        # ðŸš€ PERFORMANCE FIX: Parallelize token estimation and database operations
        output_tokens = estimate_response_tokens(ai_response["response"], ai_response.get("model_used", message_data.model))
        total_tokens = input_tokens + output_tokens
        
        # Track token usage and prepare database operation in parallel
        import asyncio
        
        # Prepare database operation
        async def prepare_db_operation():
            return await get_database()
        
        # Run usage tracking and DB preparation in parallel
        usage_task = track_usage(
            tokens=total_tokens,
            model=ai_response.get("model_used", message_data.model),
            metadata={
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "agent": message_data.agent,
                "project_id": message_data.project_id,
                "message_length": len(message_data.message),
                "response_length": len(ai_response["response"])
            }
        )
        
        db_task = prepare_db_operation()
        
        # ðŸš€ PERFORMANCE FIX: Execute both in parallel
        usage_result, db = await asyncio.gather(usage_task, db_task)
        
        # Check if usage tracking failed
        if not usage_result["success"] and "limit exceeded" in usage_result.get("error", "").lower():
            raise HTTPException(
                status_code=429,
                detail={
                    "error": "Usage limit exceeded",
                    "message": usage_result["error"],
                    "tokens_used": total_tokens,
                    "upgrade_required": True
                }
            )
        
        # ðŸš€ PERFORMANCE FIX: Optimize conversation data preparation
        conversation_data = {
            "_id": f"conv_{uuid.uuid4().hex[:12]}",
            "user_id": user_id,
            "project_id": message_data.project_id,
            "messages": [
                {
                    "id": f"user_{message_id}",
                    "content": message_data.message,
                    "sender": "user",
                    "timestamp": datetime.utcnow(),
                    "model": message_data.model,
                    "agent": message_data.agent
                },
                {
                    "id": f"ai_{message_id}",
                    "content": ai_response["response"],
                    "sender": "assistant",
                    "timestamp": datetime.utcnow(),
                    "model": ai_response.get("model_used", message_data.model),
                    "agent": message_data.agent,
                    "metadata": {
                        **ai_response.get("metadata", {}),
                        "tokens_used": total_tokens,
                        "input_tokens": input_tokens,
                        "output_tokens": output_tokens,
                        "optimized": True
                    }
                }
            ],
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        # ðŸš€ PERFORMANCE FIX: Non-blocking database operation
        if message_data.conversation_id:
            # Update existing conversation (fire and forget for speed)
            asyncio.create_task(db.conversations.update_one(
                {"_id": message_data.conversation_id, "user_id": user_id},
                {
                    "$push": {"messages": conversation_data["messages"]},
                    "$set": {"updated_at": datetime.utcnow()}
                }
            ))
        else:
            # Create new conversation (fire and forget for speed)
            asyncio.create_task(db.conversations.insert_one(conversation_data))
        
        return {
            "response": ai_response["response"],
            "model_used": ai_response.get("model_used", message_data.model),
            "agent": message_data.agent,
            "confidence": ai_response.get("confidence", 0.95),
            "suggestions": ai_response.get("suggestions", []),
            "metadata": {
                **ai_response.get("metadata", {}),
                "tokens_used": total_tokens,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "remaining_tokens": usage_result.get("remaining", "unlimited"),
                "usage_tracked": usage_result["success"],
                "performance_optimized": True,  # Indicator that optimizations are active
                "response_time_target": "<2s"
            },
            "conversation_id": message_data.conversation_id or conversation_data["_id"],
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        # Re-raise HTTP exceptions as-is
        raise
    except Exception as e:
        logger.error(f"OPTIMIZED Groq AI chat error: {e}")
        # Return structured error response
        raise HTTPException(status_code=500, detail={
            "error": "Internal service error",
            "message": "An unexpected error occurred while processing your request",
            "suggestion": "Please try again or contact support if the issue persists",
            "error_id": f"err_{uuid.uuid4().hex[:8]}"
        })

@router.get("/conversations")
async def get_conversations(
    current_user: User = Depends(get_current_user),
    project_id: Optional[str] = None,
    limit: int = 20
):
    """Get user conversations"""
    try:
        db = await get_database()
        
        query = {"user_id": str(current_user.id)}
        if project_id:
            query["project_id"] = project_id
        
        conversations_cursor = db.conversations.find(query).sort("updated_at", -1).limit(limit)
        conversations = await conversations_cursor.to_list(length=limit)
        
        # Get messages from latest conversation for immediate display
        messages = []
        if conversations and not project_id:
            latest_conversation = conversations[0]
            messages = latest_conversation.get("messages", [])
        elif project_id:
            # Get all messages for this project
            project_conversations = [conv for conv in conversations if conv.get("project_id") == project_id]
            if project_conversations:
                messages = project_conversations[0].get("messages", [])
        
        return {
            "conversations": conversations,
            "messages": messages,
            "total": len(conversations)
        }
        
    except Exception as e:
        logger.error(f"Conversations fetch error: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch conversations")

@router.get("/conversations/{conversation_id}")
async def get_conversation(
    conversation_id: str,
    current_user: User = Depends(get_current_user)
):
    """Get specific conversation"""
    try:
        db = await get_database()
        
        conversation = await db.conversations.find_one({
            "_id": conversation_id,
            "user_id": str(current_user.id)
        })
        
        if not conversation:
            raise HTTPException(status_code=404, detail="Conversation not found")
        
        return conversation
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Conversation fetch error: {e}")
        raise HTTPException(status_code=500, detail="Failed to fetch conversation")

@router.post("/conversations")
async def create_conversation(
    conversation_data: ConversationCreate,
    current_user: User = Depends(get_current_user)
):
    """Create new conversation"""
    try:
        db = await get_database()
        
        conversation = {
            "_id": f"conv_{uuid.uuid4().hex[:12]}",
            "user_id": str(current_user.id),
            "project_id": conversation_data.project_id,
            "title": conversation_data.title,
            "messages": [],
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }
        
        await db.conversations.insert_one(conversation)
        
        return conversation
        
    except Exception as e:
        logger.error(f"Conversation creation error: {e}")
        raise HTTPException(status_code=500, detail="Failed to create conversation")

@router.get("/models")
async def get_available_models():
    """Get available Groq AI models with cost information"""
    try:
        # Get models from the new Groq service
        models = await ai_service.get_available_models()
        model_status = await ai_service.get_model_status()
        
        return {
            "models": models,
            "status": model_status,
            "total": len(models),
            "ultra_fast": True,
            "cloud_based": True,
            "cost_optimized": True,
            "provider": "Groq",
            "features": {
                "ultra_fast_inference": True,
                "smart_cost_routing": True,
                "multiple_models": True,
                "enterprise_reliability": True,
                "generous_free_tier": True
            }
        }
    except Exception as e:
        logger.error(f"Failed to get models: {e}")
        return {
            "models": [],
            "status": {"error": str(e)},
            "total": 0,
            "unlimited_usage": True,
            "local_processing": True
        }

@router.get("/agents")
async def get_available_agents():
    """Get available AI agents optimized for Groq models"""
    return {
        "agents": [
            {
                "id": "developer",
                "name": "Developer Agent",
                "icon": "ðŸ’»",
                "description": "Expert in coding, debugging, and software architecture using ultra-fast Groq AI",
                "capabilities": ["Full-stack development", "Code review", "Architecture", "Debugging"],
                "recommended_model": "llama-3.1-70b-versatile",
                "ultra_fast": True,
                "cloud_based": True,
                "cost_optimized": True
            },
            {
                "id": "designer",
                "name": "Designer Agent", 
                "icon": "ðŸŽ¨",
                "description": "UI/UX design specialist with modern design principles using Groq AI",
                "capabilities": ["UI/UX Design", "Design Systems", "User Research", "Prototyping"],
                "recommended_model": "llama-3.1-8b-instant",
                "ultra_fast": True,
                "cloud_based": True,
                "cost_optimized": True
            },
            {
                "id": "tester",
                "name": "QA Agent",
                "icon": "ðŸ§ª", 
                "description": "Quality assurance and testing specialist using ultra-fast Groq AI",
                "capabilities": ["Test Strategy", "Automation", "Bug Analysis", "Performance Testing"],
                "recommended_model": "llama-3.1-70b-versatile",
                "ultra_fast": True,
                "cloud_based": True,
                "cost_optimized": True
            },
            {
                "id": "integrator",
                "name": "Integration Agent",
                "icon": "ðŸ”—",
                "description": "Third-party integration and API specialist using Groq AI", 
                "capabilities": ["API Integration", "Third-party Services", "Data Migration", "System Architecture"],
                "recommended_model": "llama-3.1-70b-versatile",
                "ultra_fast": True,
                "cloud_based": True,
                "cost_optimized": True
            },
            {
                "id": "analyst",
                "name": "Business Analyst",
                "icon": "ðŸ“Š",
                "description": "Business requirements and data analysis expert using Groq AI",
                "capabilities": ["Requirements Analysis", "Data Analysis", "Process Optimization", "Reporting"],
                "recommended_model": "mixtral-8x7b-32768",
                "ultra_fast": True,
                "cloud_based": True,
                "cost_optimized": True
            }
        ]
    }

@router.post("/models/{model_name}/download")
async def download_model(
    model_name: str,
    current_user: User = Depends(get_current_user)
):
    """Download a specific model"""
    try:
        result = await ai_service.download_model(model_name)
        return result
    except Exception as e:
        logger.error(f"Model download error: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download model: {str(e)}")

@router.get("/status")
async def get_ai_status():
    """Get Groq AI service status with FIXED connection reporting"""
    try:
        model_status = await ai_service.get_model_status()
        usage_stats = ai_service.get_usage_stats()
        is_connected = model_status.get("groq_connected", False)
        
        return {
            "service": "Groq AI",
            "status": "online" if is_connected else "offline", 
            "ultra_fast": True,
            "cloud_based": True,
            "cost_optimized": True,
            "provider": "Groq",
            "models": model_status,
            "usage": usage_stats,
            "connection_details": {
                "groq_connected": is_connected,
                "api_key_configured": model_status.get("api_key_present", False),
                "total_models": model_status.get("total_available", 0),
                "last_check": model_status.get("last_status_check", "unknown")
            },
            "features": {
                "ultra_fast_inference": True,
                "smart_cost_routing": True,
                "generous_free_tier": True,
                "enterprise_reliability": True,
                "multiple_model_support": True,
                "real_time_streaming": True,
                "cost_effective": True,
                "no_infrastructure_needed": True
            },
            "benefits": {
                "speed": "10x faster than competitors",
                "cost": "80%+ savings vs alternatives", 
                "reliability": "Enterprise-grade uptime",
                "scalability": "Handles unlimited concurrent users"
            },
            "diagnostics": {
                "service_initialized": ai_service.initialized if hasattr(ai_service, 'initialized') else True,
                "models_available": list(model_status.get("models", {}).keys()) if model_status.get("models") else [],
                "error": model_status.get("error") if not is_connected else None
            }
        }
    except Exception as e:
        logger.error(f"Status check error: {e}")
        return {
            "service": "Local Ollama AI",
            "status": "error",
            "error": str(e),
            "unlimited": True,
            "local": True,
            "diagnostics": {
                "error_type": type(e).__name__,
                "error_details": str(e)
            }
        }